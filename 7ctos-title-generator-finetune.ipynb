{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterhanlon/public-notebooks/blob/main/7ctos-title-generator-finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![7cos](https://7ctos.com/wp-content/uploads/2022/01/7CTO-Logo.png)\n",
        "\n",
        "# Fine-tuning a model to create a document titles from a body of text\n",
        "Pete Hanlon 7/6/2023\n",
        "\n",
        "Load the workbook in Google Colab [https://colab.research.google.com/]. Before you start running the workbook check that the \"Runtime\" is set to a GPU, this should happen automatically for this notebook. \n",
        "\n",
        "The training will take about an hour and a half using the free colab. If you have a pro-account you can speed things up significantly by increasing the batch size parameter to 8 in the cell that defines Seq2SeqTrainingArguments."
      ],
      "metadata": {
        "id": "EYW_BFhjdQ-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First install 3rd party service dependencies"
      ],
      "metadata": {
        "id": "FlGZ4XnU5Vmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub wandb"
      ],
      "metadata": {
        "id": "FgVf8jxQ5PAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login to Huggingface with your access token\n",
        "If you dont have an account on huggingface, you should create one so that you can upload your model once its been trained (it's free)\n",
        "\n",
        "If you dont have a Huggingface access token create a write token https://huggingface.co/settings/tokens this will allow you to upload your model once it's been finetuned"
      ],
      "metadata": {
        "id": "51aqAAgJdYsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "4wsyujds2UiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start wandb (wandb.ai) this will log training progress in the wandb.ai website, you will need an account and API access code (it's free)"
      ],
      "metadata": {
        "id": "rAOy2yIRLmUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import os\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "V6q_DArOAztp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MTw5pPNF_Nw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the environment"
      ],
      "metadata": {
        "id": "nv0b0xb4v4-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check we have all of the GPU memory available.\n",
        "\n",
        "On an A100 GPU you should see something like \"0MiB / 40960MiB\" which indicates all the GPU memory is free"
      ],
      "metadata": {
        "id": "BJwIX71Fv9kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "vZgdm_yxAjDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the necessary Python dependencies"
      ],
      "metadata": {
        "id": "ab9CuxBtiAfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers accelerate rouge-score nltk wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIzr4TE4dBhJ"
      },
      "source": [
        "Install git large file support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ricHdNBLdBhJ"
      },
      "outputs": [],
      "source": [
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install nltk sentence tokenizer, used by the HF Tokenizer"
      ],
      "metadata": {
        "id": "-mQcJHeSLQ38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "LQYmff08z070"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set key variables used during training"
      ],
      "metadata": {
        "id": "pnS-YxMRLduT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8                        # Number of samples processed before the model is updated. If you are getting OOM errors you can reduce this value to 4,2 or 1.\n",
        "num_train_epochs=4                    # The number of epochs to train for, that's the number of times the trainer cycles through the training data.\n",
        "base_model = \"google/flan-t5-base\"    # The name of the base model used to finetune, in this example its flan-t5-base\n",
        "finetuned_model_name='nodissasemble/7CTOs-document-title-generator' # The name of the final finetuned model, change \"nodissasemble\" to your huggingface username"
      ],
      "metadata": {
        "id": "hRxRXPN-A6hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine if we have a GPU to use, this should also work on a CPU but you're likely to be dead by the time the training completes :)"
      ],
      "metadata": {
        "id": "Lz_J6blmL6Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nogkbkfcB9MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YgFyhmtW_RUK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Load the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the raw training, test and validation files from github. Downloading the training data from github using curl isn't how I would normally load training data but I wanted to make it obvious that you could use your own files. A neater solution is to upload your training data to Huggingface and download from there."
      ],
      "metadata": {
        "id": "qNNyr8WtVdCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/peterhanlon/public-notebooks/main/data/title-train.jsonl > title-train.jsonl\n",
        "!curl https://raw.githubusercontent.com/peterhanlon/public-notebooks/main/data/title-validation.jsonl > title-validation.jsonl\n",
        "!curl https://raw.githubusercontent.com/peterhanlon/public-notebooks/main/data/title-test.jsonl > title-test.jsonl"
      ],
      "metadata": {
        "id": "zqNYrHZEVExS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "Load the training, validation and test files into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "data_files = {\n",
        "    \"train\": \"./title-train.jsonl\",\n",
        "    \"validation\": \"./title-validation.jsonl\",\n",
        "    \"test\": \"./title-test.jsonl\",\n",
        "}\n",
        "\n",
        "raw_datasets = load_dataset(\"json\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "FYI - The dataset object itself is DatasetDict, which contains one key for the training, validation and test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWiVUF0jIrIv"
      },
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "FYI - Dump a single row of training data so that you can see what it looks like. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6HrpprwIrIz"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_BjEBWZI_TYt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "# Preprocess the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed the training data to our model, we need to preprocess it. This is done by a Tokenizer which will tokenize the inputs and put it in a format the model expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S51rZQqdBhU"
      },
      "source": [
        "Create a method that will take a single row of data {'text':'','summary':'','title':''} and tokenize the document and title.\n",
        "\n",
        "Prefix the text \"title:\" on to the document, this will allow the model to accept prompts \"title: <text to summarize>\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"title:\" + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"title\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "FYI - Tokenize a single row so you can see what it looks like\n",
        "\n",
        "*   input_ids is the tokenized document\n",
        "*   labels is the tokenized title\n",
        "*   attention_mask a binary tensor that indicates if the input_ids is relevant or not\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b70jh26IrJS"
      },
      "outputs": [],
      "source": [
        "preprocess_function(raw_datasets['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now tokenize all the training data. Loop through the dataset tokenizing each row by calling the preprocess_function. Setting batched to true speeds up this process this is what will be used for training"
      ],
      "metadata": {
        "id": "4O6BWhvLhM90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WXUkSx1v_Vta"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "# Fine-tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our training data has been tokenized its ready, we can now download the pretrained base model and fine-tune it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(base_model).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "Next we define the Seq2SeqTrainingArguments, this is a class that contains all the attributes to customize the training. \n",
        "\n",
        "batch_size is the number of sampled processed before the model is updated. Reduce this number if you are getting CUDA OOM errors\n",
        "\n",
        "Evaluation_strategy 'epoch' tells the trainer to evaluate the model after each epoch so in this case 4 times\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GkcOjeEdBhW"
      },
      "outputs": [],
      "source": [
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir = finetuned_model_name,            # Where to output the final model and checkpoints\n",
        "    evaluation_strategy = \"epoch\",                # The model is evaluated at the end of each epoch\n",
        "    per_device_train_batch_size=batch_size,       # Number of samples to process \n",
        "    per_device_eval_batch_size=batch_size,        # Number of samples to process \n",
        "    num_train_epochs=num_train_epochs,            # Number of times the trainer will run through the training set\n",
        "    predict_with_generate=True,                   # Generates text from the model during evaluation - used to calc ROGUE score\n",
        "    save_strategy=\"epoch\",                        # Save checkpoints after each epoch\n",
        "    load_best_model_at_end=True,                  # Loads the best model at the end of the training process so we save the best model\n",
        "    report_to=\"wandb\",                            # Sends training metrics to wandb.ai\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a data collector this will be used to compute metrics"
      ],
      "metadata": {
        "id": "-i0XqWhOO9_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uoOzSxQdBhW"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "Load ROUGE as the metric we will use to measure our models performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"rouge\")"
      ],
      "metadata": {
        "id": "1LiGO7lKmJj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is boilerplate code, taken directly from Huggingface to process metrics. it just works"
      ],
      "metadata": {
        "id": "0QcbtFQkPMD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Now we build a Seq2SeqTrainer as T5 is a Seq2Seq model (Encoder/Decoder model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,                                              # The base model to finetune\n",
        "    args,                                               # Training arguments\n",
        "    train_dataset=tokenized_datasets[\"train\"],          # The tokenized training data\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],      # The tokenized validation data\n",
        "    data_collator=data_collator,                        \n",
        "    tokenizer=tokenizer,                                \n",
        "    compute_metrics=compute_metrics                     \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the train method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhmKBtVJdBhX"
      },
      "source": [
        "Now upload the finetuned model to the Huggingface Hub, once uploaded you will be able to see it in the huggingface model hub (https://huggingface.co/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HcwpyXNdBhY"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QYt0ex9g_YNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform inference using the new model"
      ],
      "metadata": {
        "id": "SmIdWMqOPnGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now load your finetuned model which will download from Huggingface and perform inference on it, fancy term for using the model.\n",
        "\n",
        "At this point you can embed the code in the cells below to use your model in your own code."
      ],
      "metadata": {
        "id": "Txh8D6ey0s8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "title_generator = pipeline(\"text2text-generation\", model=finetuned_model_name, tokenizer=finetuned_model_name)"
      ],
      "metadata": {
        "id": "RrdXfqx604I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some text takem from the BBC about AI"
      ],
      "metadata": {
        "id": "eFcHzpytSG5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='''title: \n",
        "The Terminator sci-fi film franchise envisages a malevolent AI \"Skynet\" system bent on humanity's destruction.\n",
        "\n",
        "Last week several firms warned AI could pose a threat to human existence.\n",
        "\n",
        "Prime Minister Rishi Sunak is about to travel to the US where AI is one of the items he will be discussing.\n",
        "\n",
        "AI describes the ability of computers to perform tasks typically requiring human intelligence.\n",
        "\n",
        "When it came to AI, there was a \"dystopian point of view that we can follow here. There's also a utopian point of view. Both can be possible\", Mr Scully told the TechUK Tech Policy Leadership Conference in Westminster.\n",
        "\n",
        "A dystopia is an imaginary place in which everything is as bad as possible.\n",
        "\n",
        "\"If you're only talking about the end of humanity because of some, rogue, Terminator-style scenario, you're going to miss out on all of the good that AI is already functioning - how it's mapping proteins to help us with medical research, how it's helping us with climate change.\n",
        "\n",
        "\"All of those things it's already doing and will only get better at doing.\"\n",
        "\n",
        "The government recently put out a policy document on regulating AI which was criticised for not establishing a dedicated watchdog, and some think additional measures may eventually needed to deal with the most powerful future systems .\n",
        "\n",
        "Marc Warner, a member of the AI Council, an expert body set up to advise the government, told BBC News last week a ban on the most powerful AI may be necessary.\n",
        "\n",
        "However, he argued that \"narrow AI\" designed for particular tasks, such as systems that look for cancer in medical images, should be regulated on the same basis as existing tech.\n",
        "\n",
        "Responding to reports on the possible dangers posed by AI, the prime minister's spokesperson said: \"We are not complacent about the potential risks of AI, but it also provides significant opportunities.\n",
        "\n",
        "\"We can not proceed with AI without the guard rails in place.\"\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "qiMCZ83J1NWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the text to the model and generate a title for this article"
      ],
      "metadata": {
        "id": "hBIcIsReP5dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_generator(text, num_beams=1, do_sample=False, truncation=True, min_length=1, max_length=200)"
      ],
      "metadata": {
        "id": "F0CzO3NC1LMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}